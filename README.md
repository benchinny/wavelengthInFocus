**Analysis**:

First of all, some of the following code will require ISETBIO to run, so you will need to download it. The version of ISETBIO that was used for this project (and which has diverged from the main branch) is here: https://github.com/benchinny/isetbioChinVersion

Before running any analyses, navigate to the ‘wavelengthInFocus’ repository (after downloading it, of course) and add the various folders within it to the path. Some of these are folders within the wavelengthInFocus repository itself: 

addpath(genpath('ARCsignalQualityModeling'))
addpath(genpath('acuityAnalysis'))
addpath(genpath('loadFuncs'))
addpath(genpath('miscHelperFuncs'))
addpath(genpath('plotHelperFuncs'))
addpath(genpath('psyfuncs'))

Then, add ISETBIO separately (use your own local path): 

addpath(genpath('\Users\bmccis\OneDrive - rit.edu\Documents\isetbioChinVersion\'))

All functions in the code have been designed such that they take in, as an input parameter, the file path to where you extracted the data locally. The variable is always called ‘dataPath’, and here’s an example of how it is defined for the local machine of one of the manuscript’s authors: 

dataPath = 'C:\Users\bmccis\OneDrive - rit.edu\Documents\wavelengthInFocusData\';

In the above file path, the folder ‘wavelengthInFocusData’ contains the ‘data’ folder that was downloaded from Zenodo. IMPORTANT: Make sure the folder is actually named ‘data’! To make things easiest for yourself, you could use the ‘Find Files’ functionality in MATLAB to replace the string above in all scripts with your local path.

**Data preprocessing (illustrated in Figure 7)**

A wrapper script called ‘ARCnlz_preprocess’ performs two preprocessing steps (fits LCA functions and contrast thresholds for each observer) that are described below. 

Analyzing psychophysical calibration data for acuity and LCA experiments:
The script that loads data from the calibration measurements and determines contrast thresholds for all subjects is called: ARCnlz_contrastThresholdsAll. A function called ‘ARCnlz_contrastThresholds’ loads data from experiment blocks used to ‘calibrate’ the stimulus based on participants’ contrast detection thresholds. Both functions can be found in the folder ‘ARCacuityAnalysis.’ The results of this function were used to set up the stimuli during the experiment itself, and it will be called again when doing the retinal image modelling for the acuity task.

Analyzing psychophysical data from LCA experiment (Experiment 2):
The script that loads data from the LCA experiment and fits LCA curves for all subjects is called: ARCnlzLCAall in the ‘acuityAnalysis’ folder. It calls on the function ARCnlzLCA that does the fitting per subject. 

**Figure 3**

To directly make Figure 3 from the paper, use the function ARCnlz_Fig3. This loads a MATLAB struct ‘allExp1DataRGB.mat’ that was pre-generated by two functions described below (Preprocess and average…). 

For running the statistical tests (i.e. linear regression, test of significance), the script is: ‘ARCstatisticalTestMainExp’. This script also needs to load the ‘allExp1DataRGB.mat’ struct. 

Preprocess and average accommodative data from Experiment 1 (i.e. Zernike coefficients):

Since ‘allExp1DataRGB.mat’ takes a while to create, we have included a pre-created version of it in the folder presavedFigureData. The script ‘ARCnlz_mainExpAvg’ creates ‘allExp1DataRGB.mat.’ Within ‘ARCnlz_mainExpAvg’, the function ‘ARCnlz_mainExpCalcWvInFocus’ actually does the work of loading, applying the LCA curves, and then averaging the time series values according to condition (condition is defined by the combination of stimulus distance and color in RGB coordinates). 

**Figure 4**

To recreate Figure 4B from the paper, use the script ARCnlz_Fig4B. This script loads pre-generated data. The file of pre-generated data you should load depends on which model you want to look at. A large block of comments near the top of the script will tell you the different file names you need to load for different model results. As for the pre-generated data, it can be made using ARCnlz_Fig4prep. This function takes the prefitted model fit parameters (see below), generates the predictions (function is: ARCwvInFocusModelSort, which is called by the wrapper function ARCwvInFocusModelFitAll–both functions are in the ‘ARCsignalQualityModeling’ folder), and organizes them for plotting along with the data. At the beginning of ARCnlz_Fig4prep, you can change the parameters ‘modelType’ (which color opponent model) and ‘sigQualType’ (which signal quality metric) to specify which model you want to generate the predictions for. Options are indicated in the comments.  

To recreate Figure 4C, use the script ARCnlz_Fig4C. The script loads pre-generated AIC values for making the figure from ARCnlz_Fig4prep. In ARCnlz_Fig4C, see in the comments at the beginning the other two options for making the AIC comparisons (shown in the supplementary figures). 

To look at a panel of all individual participants’ data with fits, run the function ‘ARCindvDataAndFits’. This script needs to load presaved data with filenames indicated in the comments.

**Stats associated with Figures 3 and 4**

To generate the stats reported in the paper that are associated with Figures 3 and 4, run the script ARCnlz_Fig3and4stats.m. The script will print out the stats in the command window. 

**Image quality modeling and fitting**

Note that for the image quality modeling code, I created a branch of ISETBIO in which I modified some of its functions. The original ISETBIO master branch has since diverged, so to run my code, make sure you’re using the snapshot I put on GitHub ( https://github.com/benchinny/isetbioChinVersion ). 

The following functions are all found in the ‘ARCsignalQualityModeling’ folder.

The main script for creating and saving the cone images is ARCconeImgGeneration. The images are already in the ‘data/coneImages’ folders, so there is no need to regenerate them for specific analyses unless that is the specific intention, since generating them takes many hours (~12 hours for all 8 subjects with a 16-core computer, ~24 hours with an 8-core computer). 

For fitting cone weights to the accommodation data from Experiment 1, the function is: ‘ARCwvInFocusModelFit’. The fitted weights have already been pre-saved in the ‘data/coneWeightsErrorSpatFilter/colorMechPredictions/’ folder as .mat files containing the string ‘wvInFocusModelResults’. If you rerun this function to regenerate the weights, it takes about 1 hour per subject with an 8-core computer. It calls a subfunction called ARCwvInFocusModelHelper that takes in a set of weights and cone images and determines the wavelength in focus that maximizes retinal image quality. The function is also used to fit parameters for the Strehl and Finch models shown in the supplement. 

**Figure 5**

Analyzing psychophysical data from acuity experiment (Experiment 3)

To plot the predictions (now involving image quality modeling) together with actual performance in the acuity task for all participants (e.g. Figure 5B, but for all participants), use the function ARCnlz_Fig5B.m. This folder contains d-prime values that have been pre-saved for each participant, since generating these values takes a while (simulating retinal image of grating for different amounts of defocus). The function that pre-generates these d-prime values (found in the ‘acuityAnalysis’ folder) is ARCacuityModelPrediction, which is called by the wrapper script ARCacuityModelPredictionAll. It takes around 20 minutes to generate these d-prime values for each subject. 

To generate Figure 5C, run the script ARCnlz_Fig5C. This function will also print out, in the command line, stats reported in the paper associated with this figure. 

To recreate Figure 5D, use the function ARCnlz_Fig5D. This function will also print out, in the command line, stats reported in the paper associated with this figure. 

**Figure 6**

The script ‘ARCmakeFig6’ makes the images in Figure 6 from the manuscript. This is more of a conceptual figure so there’s not much to say here other than it displays cone and mechanism images at various wavelength-in-focus of interest.
